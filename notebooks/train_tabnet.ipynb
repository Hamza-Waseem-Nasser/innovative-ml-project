{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25416d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be9dfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded transactions: (779495, 10)\n"
     ]
    }
   ],
   "source": [
    "# 1) Load the cleaned transactions\n",
    "df = pd.read_csv(\"../data/transactions_clean.csv\", parse_dates=[\"InvoiceDate\"])\n",
    "print(\"Loaded transactions:\", df.shape)\n",
    "\n",
    "# 2) Feature Engineering\n",
    "# ──────────────────────\n",
    "# Extract time features\n",
    "df[\"hour\"] = df[\"InvoiceDate\"].dt.hour\n",
    "df[\"day_of_week\"] = df[\"InvoiceDate\"].dt.dayofweek\n",
    "df[\"month\"] = df[\"InvoiceDate\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "add15151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 779495 entries, 0 to 779494\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   Invoice      779495 non-null  int64         \n",
      " 1   StockCode    779495 non-null  object        \n",
      " 2   Description  779495 non-null  object        \n",
      " 3   Quantity     779495 non-null  int64         \n",
      " 4   InvoiceDate  779495 non-null  datetime64[ns]\n",
      " 5   Price        779495 non-null  float64       \n",
      " 6   Customer ID  779495 non-null  float64       \n",
      " 7   Country      779495 non-null  object        \n",
      " 8   TotalPrice   779495 non-null  float64       \n",
      " 9   Month        779495 non-null  object        \n",
      " 10  hour         779495 non-null  int32         \n",
      " 11  day_of_week  779495 non-null  int32         \n",
      " 12  month        779495 non-null  int32         \n",
      "dtypes: datetime64[ns](1), float64(3), int32(3), int64(2), object(4)\n",
      "memory usage: 68.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcc8e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "target = \"TotalPrice\"\n",
    "categorical_cols = [\"StockCode\", \"Country\"]\n",
    "numerical_cols   = [\"Quantity\", \"Price\", \"hour\", \"day_of_week\", \"month\"]\n",
    "features = categorical_cols + numerical_cols\n",
    "\n",
    "# 3) Encode categoricals\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    joblib.dump(le, f\"../models/enc_{col}.pkl\")  # save encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16f0ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Prepare data arrays\n",
    "X = df[features].values\n",
    "y = df[target].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ba6660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (623596, 7) Test size: (155899, 7)\n"
     ]
    }
   ],
   "source": [
    "# 5) Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e84bc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/scaler_num.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Scale numerical features\n",
    "#    We scale only the numerical part of X\n",
    "num_start = len(categorical_cols)\n",
    "scaler = StandardScaler()\n",
    "X_train[:, num_start:] = scaler.fit_transform(X_train[:, num_start:])\n",
    "X_test[:, num_start:]  = scaler.transform(X_test[:, num_start:])\n",
    "joblib.dump(scaler, \"../models/scaler_num.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3515cec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Innovative-ml-project\\NEW-ML-PROJECT\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "c:\\Users\\hp\\Innovative-ml-project\\NEW-ML-PROJECT\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.41352 |  0:00:48s\n",
      "epoch 1  | loss: 1.38137 |  0:01:31s\n",
      "epoch 2  | loss: 1.3646  |  0:02:14s\n",
      "epoch 3  | loss: 1.32862 |  0:02:58s\n",
      "epoch 4  | loss: 1.28311 |  0:03:38s\n",
      "epoch 5  | loss: 1.22697 |  0:04:23s\n",
      "epoch 6  | loss: 1.22381 |  0:05:05s\n",
      "epoch 7  | loss: 1.17268 |  0:05:46s\n",
      "epoch 8  | loss: 1.14406 |  0:06:26s\n",
      "epoch 9  | loss: 1.10131 |  0:07:06s\n",
      "epoch 10 | loss: 1.0797  |  0:07:48s\n",
      "epoch 11 | loss: 1.11537 |  0:08:29s\n",
      "epoch 12 | loss: 1.06347 |  0:09:09s\n",
      "epoch 13 | loss: 1.07211 |  0:09:50s\n",
      "epoch 14 | loss: 1.08134 |  0:10:29s\n",
      "epoch 15 | loss: 1.05163 |  0:11:09s\n",
      "epoch 16 | loss: 1.04063 |  0:11:49s\n",
      "epoch 17 | loss: 1.05251 |  0:12:30s\n",
      "epoch 18 | loss: 1.03585 |  0:13:12s\n",
      "epoch 19 | loss: 1.02945 |  0:13:57s\n",
      "epoch 20 | loss: 1.02196 |  0:14:42s\n",
      "epoch 21 | loss: 1.02766 |  0:15:24s\n",
      "epoch 22 | loss: 1.01633 |  0:16:05s\n",
      "epoch 23 | loss: 1.0159  |  0:16:46s\n",
      "epoch 24 | loss: 1.01559 |  0:17:29s\n",
      "epoch 25 | loss: 1.0008  |  0:18:13s\n",
      "epoch 26 | loss: 1.00599 |  0:18:57s\n",
      "epoch 27 | loss: 1.00819 |  0:19:38s\n",
      "epoch 28 | loss: 1.01347 |  0:20:21s\n",
      "epoch 29 | loss: 1.00323 |  0:21:03s\n",
      "epoch 30 | loss: 1.00926 |  0:21:45s\n",
      "epoch 31 | loss: 1.00615 |  0:22:30s\n",
      "epoch 32 | loss: 1.00296 |  0:23:11s\n",
      "epoch 33 | loss: 1.00816 |  0:23:52s\n",
      "epoch 34 | loss: 1.00061 |  0:24:34s\n",
      "epoch 35 | loss: 1.00425 |  0:25:15s\n",
      "epoch 36 | loss: 1.00661 |  0:25:56s\n",
      "epoch 37 | loss: 1.00494 |  0:26:38s\n",
      "epoch 38 | loss: 1.00045 |  0:27:20s\n",
      "epoch 39 | loss: 0.99749 |  0:28:01s\n",
      "epoch 40 | loss: 0.99712 |  0:28:43s\n",
      "epoch 41 | loss: 0.99873 |  0:29:25s\n",
      "epoch 42 | loss: 1.00225 |  0:30:07s\n",
      "epoch 43 | loss: 0.99256 |  0:30:48s\n",
      "epoch 44 | loss: 0.99783 |  0:31:28s\n",
      "epoch 45 | loss: 0.99992 |  0:32:10s\n",
      "epoch 46 | loss: 0.99772 |  0:32:50s\n",
      "epoch 47 | loss: 1.00041 |  0:33:32s\n",
      "epoch 48 | loss: 0.99693 |  0:34:14s\n",
      "epoch 49 | loss: 0.99816 |  0:34:55s\n",
      "epoch 50 | loss: 0.99767 |  0:35:35s\n",
      "epoch 51 | loss: 0.99227 |  0:36:16s\n",
      "epoch 52 | loss: 0.99515 |  0:36:56s\n",
      "epoch 53 | loss: 1.00059 |  0:37:37s\n",
      "epoch 54 | loss: 0.99983 |  0:38:18s\n",
      "epoch 55 | loss: 0.99707 |  0:38:59s\n",
      "epoch 56 | loss: 0.99942 |  0:39:39s\n",
      "epoch 57 | loss: 0.99945 |  0:40:20s\n",
      "epoch 58 | loss: 0.99597 |  0:41:01s\n",
      "epoch 59 | loss: 0.99797 |  0:41:41s\n",
      "epoch 60 | loss: 0.99669 |  0:42:23s\n",
      "epoch 61 | loss: 1.00005 |  0:43:03s\n",
      "epoch 62 | loss: 0.9977  |  0:43:43s\n",
      "epoch 63 | loss: 0.99689 |  0:44:24s\n",
      "epoch 64 | loss: 0.99738 |  0:45:05s\n",
      "epoch 65 | loss: 0.9956  |  0:45:46s\n",
      "epoch 66 | loss: 0.99479 |  0:46:27s\n",
      "epoch 67 | loss: 0.99687 |  0:47:08s\n",
      "epoch 68 | loss: 0.99353 |  0:47:48s\n",
      "epoch 69 | loss: 0.99656 |  0:48:29s\n",
      "epoch 70 | loss: 0.99811 |  0:49:09s\n",
      "epoch 71 | loss: 0.99512 |  0:49:50s\n",
      "\n",
      "Early stopping occurred at epoch 71 with best_epoch = 51 and best_loss = 0.99227\n",
      "Successfully saved model at ../models/tabnet_pretrainer.zip\n",
      "✅ Pretraining complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Innovative-ml-project\\NEW-ML-PROJECT\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# 7) Self‑Supervised Pretraining\n",
    "# ────────────────────────────────\n",
    "from pytorch_tabnet.callbacks import EarlyStopping\n",
    "\n",
    "pretrainer = TabNetPretrainer(\n",
    "    input_dim=X_train.shape[1],\n",
    "    mask_type='entmax'  # sparsity in masks\n",
    ")\n",
    "# define callbacks\n",
    "es = EarlyStopping(\n",
    "    patience=20,\n",
    "    early_stopping_metric=\"loss\",  # Metric to monitor\n",
    "    is_maximize=False              # Set to False for minimizing loss\n",
    ")\n",
    "\n",
    "# fit with callbacks and verbose logging\n",
    "pretrainer.fit(\n",
    "    X_train,\n",
    "    max_epochs=100,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    pretraining_ratio=0.8,\n",
    "    num_workers=3,\n",
    "    drop_last=False,\n",
    "    callbacks=[es]\n",
    "    \n",
    ")\n",
    "\n",
    "pretrainer.save_model(\"../models/tabnet_pretrainer\")\n",
    "print(\"✅ Pretraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "964fd047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Innovative-ml-project\\NEW-ML-PROJECT\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "# 8) Fine‑Tuning as Regressor\n",
    "# ────────────────────────────\n",
    "regressor = TabNetRegressor(\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=1,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params={\"lr\":1e-3, \"weight_decay\":1e-4},\n",
    "    scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR\n",
    ")\n",
    "\n",
    "# load pretrained encoder weights\n",
    "# Instead of regressor.load_weights_from_unsupervised(pretrainer)\n",
    "# use the following:\n",
    "regressor._set_network() # Initialize the network attribute\n",
    "regressor.load_weights_from_unsupervised(pretrainer) # Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c68689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 53433.81957| test_mse: 42200.99097|  0:00:50s\n",
      "epoch 1  | loss: 52150.70518| test_mse: 21908.58382|  0:01:40s\n",
      "epoch 2  | loss: 51396.70429| test_mse: 41916.81119|  0:02:31s\n",
      "epoch 3  | loss: 50986.27797| test_mse: 22043.13526|  0:03:22s\n",
      "epoch 4  | loss: 50586.85062| test_mse: 12192.49054|  0:04:12s\n",
      "epoch 5  | loss: 50238.15494| test_mse: 19971.6419|  0:05:03s\n",
      "epoch 6  | loss: 49989.97393| test_mse: 12521.28089|  0:05:53s\n",
      "epoch 7  | loss: 49674.9654| test_mse: 33704.25411|  0:06:43s\n",
      "epoch 8  | loss: 49562.03424| test_mse: 31834.03896|  0:07:34s\n",
      "epoch 9  | loss: 49282.25062| test_mse: 21310.37602|  0:08:25s\n",
      "epoch 10 | loss: 49140.07537| test_mse: 20963.26524|  0:09:16s\n",
      "epoch 11 | loss: 48923.82932| test_mse: 24895.83432|  0:10:07s\n",
      "epoch 12 | loss: 48784.84036| test_mse: 23444.30536|  0:10:58s\n",
      "epoch 13 | loss: 48642.10622| test_mse: 6855.4336|  0:11:48s\n",
      "epoch 14 | loss: 48509.47177| test_mse: 20266.00081|  0:12:39s\n",
      "epoch 15 | loss: 48460.18469| test_mse: 10635.60727|  0:13:31s\n",
      "epoch 16 | loss: 48271.8923| test_mse: 13598.96576|  0:14:21s\n",
      "epoch 17 | loss: 48190.18991| test_mse: 21102.58809|  0:15:12s\n",
      "epoch 18 | loss: 48005.1297| test_mse: 10076.54826|  0:16:02s\n",
      "epoch 19 | loss: 47978.76071| test_mse: 5174.60598|  0:16:53s\n",
      "epoch 20 | loss: 47843.66575| test_mse: 4721.02115|  0:17:45s\n",
      "epoch 21 | loss: 48230.38967| test_mse: 5211.63701|  0:18:36s\n",
      "epoch 22 | loss: 47648.17058| test_mse: 10740.7139|  0:19:26s\n",
      "epoch 23 | loss: 47613.01284| test_mse: 5488.47434|  0:20:17s\n",
      "epoch 24 | loss: 47556.92075| test_mse: 10649.87467|  0:21:08s\n",
      "epoch 25 | loss: 47432.58441| test_mse: 16748.0565|  0:21:59s\n",
      "epoch 26 | loss: 47378.09524| test_mse: 15164.1847|  0:22:50s\n",
      "epoch 27 | loss: 47361.90118| test_mse: 26893.68423|  0:23:41s\n",
      "epoch 28 | loss: 47371.8015| test_mse: 4771.64872|  0:24:32s\n",
      "epoch 29 | loss: 47355.62448| test_mse: 26621.92369|  0:25:23s\n",
      "epoch 30 | loss: 47169.44089| test_mse: 11247.20626|  0:26:13s\n",
      "epoch 31 | loss: 47119.87972| test_mse: 8617.68013|  0:27:03s\n",
      "epoch 32 | loss: 47288.10158| test_mse: 5369.34648|  0:27:54s\n",
      "epoch 33 | loss: 47094.40276| test_mse: 24496.75974|  0:28:46s\n",
      "epoch 34 | loss: 46990.79437| test_mse: 25706.356|  0:29:37s\n",
      "epoch 35 | loss: 47130.5167| test_mse: 25765.92648|  0:30:27s\n",
      "epoch 36 | loss: 46967.56882| test_mse: 23815.9138|  0:31:17s\n",
      "epoch 37 | loss: 46866.2609| test_mse: 11652.71713|  0:32:09s\n",
      "epoch 38 | loss: 46900.28331| test_mse: 30262.46962|  0:33:02s\n",
      "epoch 39 | loss: 47011.57423| test_mse: 13730.1451|  0:33:52s\n",
      "epoch 40 | loss: 46806.13265| test_mse: 26471.13611|  0:34:43s\n",
      "epoch 41 | loss: 46770.39446| test_mse: 9651.27117|  0:35:33s\n",
      "epoch 42 | loss: 46713.84471| test_mse: 19525.20817|  0:36:24s\n",
      "epoch 43 | loss: 47109.96143| test_mse: 8026.59939|  0:37:14s\n",
      "epoch 44 | loss: 46937.69451| test_mse: 17318.1971|  0:38:04s\n",
      "epoch 45 | loss: 47029.81759| test_mse: 18349.70928|  0:38:55s\n",
      "epoch 46 | loss: 46744.72374| test_mse: 11741.69696|  0:39:46s\n",
      "epoch 47 | loss: 46690.21155| test_mse: 4931.03818|  0:40:37s\n",
      "epoch 48 | loss: 46705.06441| test_mse: 6849.16788|  0:41:27s\n",
      "epoch 49 | loss: 46840.61051| test_mse: 14613.88771|  0:42:17s\n",
      "epoch 50 | loss: 46700.0366| test_mse: 13542.49629|  0:43:09s\n",
      "\n",
      "Early stopping occurred at epoch 50 with best_epoch = 20 and best_test_mse = 4721.02115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Innovative-ml-project\\NEW-ML-PROJECT\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit on train, evaluate on test\n",
    "regressor.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_name=['test'],\n",
    "    max_epochs=100,\n",
    "    patience=30,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=3,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327df0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at ../models/tabnet_regressor.zip\n",
      "✅ Fine‑tuning complete. Model saved to models/tabnet_regressor.zip\n"
     ]
    }
   ],
   "source": [
    "# Save the fine‑tuned model\n",
    "regressor.save_model(\"../models/tabnet_regressor\")\n",
    "print(\"✅ Fine‑tuning complete. Model saved to models/tabnet_regressor.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7dbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
